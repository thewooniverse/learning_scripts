{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "db643622-3bca-4a33-93ab-6f3b3ee511fb",
   "metadata": {},
   "source": [
    "# Section 1: Get Started "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f0f9c248-7419-47b5-a5f5-330dc056743d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.schema import HumanMessage\n",
    "\n",
    "load_dotenv()\n",
    "openai_api_key=os.getenv('OPENAI_API_KEY', 'YourAPIKey')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e1c95fd4-baae-48b5-bfe6-3159ca14158e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\\\\\\\\\n\\nHello!'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm = OpenAI(temperature=1)\n",
    "llm.predict(\"hi!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9064f1af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hello! How can I assist you today?'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_model = ChatOpenAI()\n",
    "chat_model.predict(\"hi!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f288a212",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Rainbow Socks Co.\n",
      "Colorful Soles\n",
      "<class 'str'>\n",
      "<class 'str'>\n"
     ]
    }
   ],
   "source": [
    "text = \"What would be a good company name for a company that makes colorful socks?\"\n",
    "response1 = llm.predict(text)\n",
    "response2 = chat_model.predict(text)\n",
    "print(response1)\n",
    "print(response2)\n",
    "# print(type(response1)) # <class 'str'>\n",
    "# print(type(response2)) # <class 'str'>\n",
    "# as you can tell here its string in, string out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fd8b4eab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content=\"\\n\\nSockin' It Up!\" additional_kwargs={} example=False\n",
      "content='SockSplash' additional_kwargs={} example=False\n"
     ]
    }
   ],
   "source": [
    "messages = [HumanMessage(content=text)]\n",
    "\n",
    "cr1 = llm.predict_messages(messages)\n",
    "print(cr1)\n",
    "cr2 = chat_model.predict_messages(messages)\n",
    "print(cr2)\n",
    "# here it is message in, message out - regardless of llm or chat model "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79104b33",
   "metadata": {},
   "source": [
    "\n",
    "The standard interface that LangChain provides has two methods:\n",
    "\n",
    "predict: Takes in a string, returns a string\n",
    "predict_messages: Takes in a list of messages, returns a message.\n",
    "\n",
    "For both these methods, you can also pass in parameters as keyword arguments. For example, you could pass in temperature=0 to adjust the temperature that is used from what the object was configured with. Whatever values are passed in during run time will always override what the object was configured with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "cf38b3d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content=\"J'adore la programmation.\" additional_kwargs={} example=False\n",
      "content=\"\\n\\nSystem: J'adore la programmation.\" additional_kwargs={} example=False\n",
      "<class 'langchain.schema.messages.AIMessage'>\n"
     ]
    }
   ],
   "source": [
    "from langchain.prompts.chat import ChatPromptTemplate\n",
    "\n",
    "template = \"You are a helpful assistant that translates {input_language} to {output_language}.\"\n",
    "human_template = \"{text}\"\n",
    "\n",
    "\n",
    "chat_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", template),\n",
    "    (\"human\", human_template),\n",
    "])\n",
    "# print(type(chat_prompt)) # <class 'langchain.prompts.chat.ChatPromptTemplate'>\n",
    "\n",
    "cprompt = chat_prompt.format_messages(input_language=\"English\", output_language=\"French\", text=\"I love programming.\")\n",
    "# print(type(cprompt)) # <class 'list'>\n",
    "cr3 = chat_model.predict_messages(cprompt)\n",
    "cr4 = llm.predict_messages(cprompt)\n",
    "print(cr3)\n",
    "print(cr4)\n",
    "# print(type(cr3)) # <class 'langchain.schema.messages.AIMessage'>\n",
    "# print(type(cr4)) # <class 'langchain.schema.messages.AIMessage'>\n",
    "\n",
    "# in the case of just simple prompt templates, you can just think about it like F Strings;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dfab553",
   "metadata": {},
   "source": [
    "### Output parsers\n",
    "OutputParsers convert the raw output of an LLM into a format that can be used downstream. There are few main types of OutputParsers, including:\n",
    "\n",
    "- Convert text from LLM into structured information (e.g. JSON)\n",
    "- Convert a ChatMessage into just a string\n",
    "- Convert the extra information returned from a call besides the message (like OpenAI function invocation) into a string.\n",
    "- For full information on this, see the section on output parsers.\n",
    "\n",
    "In this getting started guide, we will write our own output parser - one that converts a comma separated list into a list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a7cc576f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ayy', 'lmao - wtf??']"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.schema import BaseOutputParser\n",
    "\n",
    "class CommaSeparatedListOutputParser(BaseOutputParser):\n",
    "    \"\"\"Parse the output of an LLM call to a comma-separated list.\"\"\"\n",
    "\n",
    "    def parse(self, text: str):\n",
    "        \"\"\"Parse the output of an LLM call.\"\"\"\n",
    "        return text.strip().split(\", \")\n",
    "\n",
    "CommaSeparatedListOutputParser().parse(\"ayy, lmao - wtf??\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e4beee6",
   "metadata": {},
   "source": [
    "# PromptTemplate + LLM + OutputParser\n",
    "We can now combine all these into one chain. This chain will take input variables, pass those to a prompt template to create a prompt, pass the prompt to a language model, and then pass the output through an (optional) output parser. This is a convenient way to bundle up a modular piece of logic. Let's see it in action!\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24bafc50",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts.chat import ChatPromptTemplate\n",
    "from langchain.schema import BaseOutputParser\n",
    "\n",
    "class CommaSeparatedListOutputParser(BaseOutputParser):\n",
    "    \"\"\"Parse the output of an LLM call to a comma-separated list.\"\"\"\n",
    "\n",
    "    def parse(self, text: str):\n",
    "        \"\"\"Parse the output of an LLM call.\"\"\"\n",
    "        return text.strip().split(\", \")\n",
    "\n",
    "template = \"\"\"You are a helpful assistant who generates comma separated lists.\n",
    "A user will pass in a category, and you should generate 5 objects in that category in a comma separated list.\n",
    "ONLY return a comma separated list, and nothing more.\"\"\"\n",
    "human_template = \"{text}\"\n",
    "\n",
    "chat_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", template),\n",
    "    (\"human\", human_template),\n",
    "])\n",
    "chain = chat_prompt | ChatOpenAI() | CommaSeparatedListOutputParser()\n",
    "chain.invoke({\"text\": \"colors\"})\n",
    "# >> ['red', 'blue', 'green', 'yellow', 'orange']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36aefe23",
   "metadata": {},
   "source": [
    "Note that we are using the | syntax to join these components together. This | syntax is called the LangChain Expression Language. To learn more about this syntax, read the documentation here."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
