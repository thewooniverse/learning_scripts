{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7ce8bc84",
   "metadata": {},
   "source": [
    "# Function Calling with OpenAI's GPT Models: An Interactive Tutorial\n",
    "\n",
    "In this notebook, we'll dive deep into a powerful feature offered by the latest versions of OpenAI's GPT models (like gpt-3.5-turbo-0613 and gpt-4-0613): function calling.\n",
    "\n",
    "Let's imagine you're talking to a ChatGPT model and you want to have it use a tool. Traditionally you'd have to do some clever prompting to have it return the format you'd like.\n",
    "\n",
    "Now you can tell it about certain actions, or **\"functions\"**, it can take\n",
    "\n",
    "This doesn't mean the assistant actually performs these actions. Rather, it's aware of them and can instruct you on how to perform these actions based on the conversation at hand.\n",
    "\n",
    "For example, you can tell the assistant about a function that fetches weather data, and when asked \"What's the weather like in Boston?\", the assistant can reply with instructions on how to call this weather-fetching function with 'Boston' as the input.\n",
    "\n",
    "**Function calling** enables us to leverage the model's natural language understanding to effectively turn human language into structured data or specific function calls in our code.\n",
    "\n",
    "This capability is useful in numerous scenarios, from creating chatbots that can interact with other APIs, to automating tasks and extracting structured information from natural language inputs. See more information about [function calling](https://platform.openai.com/docs/guides/gpt/function-calling)\n",
    "\n",
    "Let's explore and start by importing our packages\n",
    "\n",
    "---\n",
    "This allows us to use LLMs as a reasoning engine. This is also the logic and the underlying mechanism that decides which plugin to use in OpenAI.\n",
    "\n",
    "\n",
    "[YOUTUBE LINK](https://www.youtube.com/watch?v=0-zlUy7VUjg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d8042fc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install langchain --upgrade\n",
    "# Version: 0.0.199 Make sure you're on the latest version\n",
    "\n",
    "import langchain\n",
    "import openai\n",
    "import json\n",
    "\n",
    "# Environment Variables\n",
    "import os\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "openai.api_key = os.getenv('OPENAI_API_KEY', 'YourAPIKeyIfNotSet')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ea3650a",
   "metadata": {},
   "source": [
    "## OpenAI Vanilla Example\n",
    "\n",
    "Let's run through OpenAI's vanilla example of calling a weather API.\n",
    "\n",
    "First let's define our functions. This is the meat and potatoes of the new update\n",
    "\n",
    "Functions are specified with the following fields:\n",
    "\n",
    "* **Name:** The name of the function.\n",
    "* **Description:** A description of what the function does. The model will use this to decide when to call the function.\n",
    "* **Parameters:** The parameters object contains all of the input fields the function requires. These inputs can be of the following types: String, Number, Boolean, Object, Null, AnyOf. Refer to the API reference docs for details.\n",
    "* **Required:** Which of the parameters are required to make a query. The rest will be treated as optional."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3bd25223",
   "metadata": {},
   "outputs": [],
   "source": [
    "function_descriptions = [\n",
    "            {\n",
    "                \"name\": \"get_current_weather\", #function name\n",
    "                \"description\": \"Get the current weather in a given location\", \n",
    "                # description -> this is the instruction that will instruct the model on which tools to pick\n",
    "\n",
    "                \n",
    "                \"parameters\": {\n",
    "                    \"type\": \"object\",\n",
    "                    \"properties\": {\n",
    "                        \"location\": {\n",
    "                            \"type\": \"string\",\n",
    "                            \"description\": \"The city and state, e.g. San Francisco, CA\",\n",
    "                        },\n",
    "                        \"unit\": {\n",
    "                            \"type\": \"string\",\n",
    "                            \"description\": \"The temperature unit to use. Infer this from the users location.\",\n",
    "                            \"enum\": [\"celsius\", \"fahrenheit\"] # can also specify enum values;\n",
    "                        },\n",
    "                    },\n",
    "                    \"required\": [\"location\", \"unit\"],\n",
    "                },\n",
    "            }\n",
    "        ]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bdb85b0",
   "metadata": {},
   "source": [
    "Then let's call the OpenAI API with this as a new parameter. Note: Make sure to use a model that can accept the function call. Here we are using `gpt-3.5-turbo-0613`.\n",
    "\n",
    "Let's first set a query that came from the user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1afea666",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_query = \"What's the weather like in London United Kingdom?\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "443aee72",
   "metadata": {},
   "source": [
    "Then let's set up our API call to OpenAI. Note: `function_call=\"auto\"` will allow the model to choose whether or not it responds with a function. You can set this to `none` if you *don't* want a function response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6a565cf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = openai.ChatCompletion.create( #vanilla openAI\n",
    "        model=\"gpt-4-0613\", # gpt4 static version\n",
    "        \n",
    "        # This is the chat message from the user\n",
    "        messages=[{\"role\": \"user\", \"content\": user_query}], # we pass it series of messages; this one is user message, which is user query\n",
    "    \n",
    "        \n",
    "        functions=function_descriptions, #we pass the list above, and its a list, so we can define multiple functions\n",
    "        function_call=\"auto\", # function call specifies whether the bot should return using a function or not.\n",
    "                              # In this case; auto, auto picks / determines whether it should be used. None makes it so that it will not be used etc...\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9344b392",
   "metadata": {},
   "source": [
    "Great, let's take a look at the response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d5067356",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"role\": \"assistant\",\n",
      "  \"content\": null,\n",
      "  \"function_call\": {\n",
      "    \"name\": \"get_current_weather\",\n",
      "    \"arguments\": \"{\\n  \\\"location\\\": \\\"London, United Kingdom\\\",\\n  \\\"unit\\\": \\\"celsius\\\"\\n}\"\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "ai_response_message = response[\"choices\"][0][\"message\"]\n",
    "print(ai_response_message)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c158975c",
   "metadata": {},
   "source": [
    "Awesome, now we have our response w/ specific arguments called out.\n",
    "\n",
    "Let's clean up our response a bit better"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "04ada914",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "London, United Kingdom celsius\n"
     ]
    }
   ],
   "source": [
    "user_location = eval(ai_response_message['function_call']['arguments']).get(\"location\")\n",
    "user_unit = eval(ai_response_message['function_call']['arguments']).get(\"unit\")\n",
    "\n",
    "print(user_location, user_unit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b6bb6a0a-3ba0-4781-a112-cc614c13b048",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"location\": \"London, United Kingdom\",\n",
      "  \"unit\": \"celsius\"\n",
      "}\n",
      "London, United Kingdom celsius\n"
     ]
    }
   ],
   "source": [
    "json_string = ai_response_message['function_call'][\"arguments\"]\n",
    "print(json_string)\n",
    "data = json.loads(json_string)\n",
    "print(data['location'], data['unit'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbc81e28",
   "metadata": {},
   "source": [
    "Then let's make a function that will serve as an interface to a dummy api call"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a7e4fe28",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_current_weather(location, unit):\n",
    "    \n",
    "    \"\"\"Get the current weather in a given location\"\"\"\n",
    "    \n",
    "    weather_info = {\n",
    "        \"location\": location,\n",
    "        \"temperature\": \"72\",\n",
    "        \"unit\": unit,\n",
    "        \"forecast\": [\"sunny\", \"windy\"],\n",
    "    }\n",
    "    return json.dumps(weather_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "505806ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "function_response = get_current_weather(\n",
    "    location=data['location'],\n",
    "    unit=data['unit'],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6c24e71c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'{\"location\": \"London, United Kingdom\", \"temperature\": \"72\", \"unit\": \"celsius\", \"forecast\": [\"sunny\", \"windy\"]}'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function_response # example response from the API"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89694ba3",
   "metadata": {},
   "source": [
    "Now that we have our reponse from our service, we can pass this information back to our model for a natural language response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2c4a5c78",
   "metadata": {},
   "outputs": [],
   "source": [
    "second_response = openai.ChatCompletion.create(\n",
    "    model=\"gpt-4-0613\",\n",
    "    \n",
    "    \n",
    "    messages=[ # message history beforehand\n",
    "        {\"role\": \"user\", \"content\": user_query}, # this is the user query\n",
    "        ai_response_message, # ai response message, represents what it sent back to us\n",
    "        {\n",
    "            \"role\": \"function\", # go and call the function\n",
    "            \"name\": \"get_current_weather\", # what function was called\n",
    "            \"content\": function_response, # the response above.\n",
    "        },\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e4020764",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The current weather in London, United Kingdom is 72 degrees celsius. The forecast indicates that it is sunny and windy.\n"
     ]
    }
   ],
   "source": [
    "print (second_response['choices'][0]['message']['content'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "519f57b4",
   "metadata": {},
   "source": [
    "## LangChain Support For Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1d3f216d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.schema import HumanMessage, AIMessage, ChatMessage\n",
    "from langchain.tools import format_tool_to_openai_function, YouTubeSearchTool, MoveFileTool"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c30ea30c",
   "metadata": {},
   "source": [
    "Let's load up our models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d4aa0e58",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(model=\"gpt-4-0613\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd72917e",
   "metadata": {},
   "source": [
    "Let's load our tools and then transform them into OpenAI's function framework"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "93b8f50a",
   "metadata": {},
   "outputs": [],
   "source": [
    "tools = [MoveFileTool()]\n",
    "functions = [format_tool_to_openai_function(t) for t in tools]\n",
    "# format_tool_to_openai_function -> translates each tool into function calling schema that openAI wants to see.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47e978c0",
   "metadata": {},
   "source": [
    "Let's take a look at what this tool was transformed as"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "4b56cdc6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'name': 'move_file',\n",
       "  'description': 'Move or rename a file from one location to another',\n",
       "  'parameters': {'title': 'FileMoveInput',\n",
       "   'description': 'Input for MoveFileTool.',\n",
       "   'type': 'object',\n",
       "   'properties': {'source_path': {'title': 'Source Path',\n",
       "     'description': 'Path of the file to move',\n",
       "     'type': 'string'},\n",
       "    'destination_path': {'title': 'Destination Path',\n",
       "     'description': 'New path for the moved file',\n",
       "     'type': 'string'}},\n",
       "   'required': ['source_path', 'destination_path']}}]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "functions\n",
    "# can see, that we have our name, parameters etc... that gives the LLM the information it needs to use those tools."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "aff6280a",
   "metadata": {},
   "outputs": [],
   "source": [
    "message = llm.predict_messages([HumanMessage(content='move file foo to bar')], functions=functions)\n",
    "# please move file foo to bar, and we pass on the functions=function -> which is the list of tool available to us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "adf61d07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'source_path': 'foo', 'destination_path': 'bar'}\n"
     ]
    }
   ],
   "source": [
    "message.additional_kwargs['function_call']\n",
    "# function_call that we get back from OpenAI\n",
    "# now as the example above, we can now parse these arguments in the same way as this one:\n",
    "\n",
    "  # \"function_call\": {\n",
    "  #   \"name\": \"get_current_weather\",\n",
    "  #   \"arguments\": \"{\\n  \\\"location\\\": \\\"London, United Kingdom\\\",\\n  \\\"unit\\\": \\\"celsius\\\"\\n}\"\n",
    "  # }\n",
    "\n",
    "# we can now parse the arguments\n",
    "data = json.loads(message.additional_kwargs['function_call']['arguments'])\n",
    "print(data)\n",
    "# now that we have data = {'source_path': 'foo', 'destination_path': 'bar'}\n",
    "# we can use these parameters to call the function, e.g.\n",
    "\n",
    "# move_file(source_path=data['source_path'], destination_path=data['destination_path'])\n",
    "# basically, at this point I have the data and now I can call whatever API that I want to.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d139846",
   "metadata": {},
   "source": [
    "### Ad Hoc Example Financial Forecast Edit"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12fd005b",
   "metadata": {},
   "source": [
    "I'm going to make a new function description that talks about updating a financial model. It'll take 3 params, year to update, category to update, and amount to update."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "fed0f289",
   "metadata": {},
   "outputs": [],
   "source": [
    "function_descriptions = [\n",
    "            {\n",
    "                \"name\": \"edit_financial_forecast\",\n",
    "                \"description\": \"Make an edit to a users financial forecast model\",\n",
    "                \"parameters\": {\n",
    "                    \"type\": \"object\",\n",
    "                    \"properties\": {\n",
    "                        \"year\": {\n",
    "                            \"type\": \"integer\",\n",
    "                            \"description\": \"The year the user would like to make an edit to their forecast for\",\n",
    "                        },\n",
    "                        \"category\": {\n",
    "                            \"type\": \"string\",\n",
    "                            \"description\": \"The category of the edit a user would like to edit\"\n",
    "                        },\n",
    "                        \"amount\": {\n",
    "                            \"type\": \"integer\",\n",
    "                            \"description\": \"The amount of units the user would like to change\"\n",
    "                        },\n",
    "                    },\n",
    "                    \"required\": [\"year\", \"category\", \"amount\"],\n",
    "                },\n",
    "            },\n",
    "            {\n",
    "                \"name\": \"print_financial_forecast\",\n",
    "                \"description\": \"Send the financial forecast to the printer\",\n",
    "                \"parameters\": {\n",
    "                    \"type\": \"object\",\n",
    "                    \"properties\": {\n",
    "                        \"printer_name\": {\n",
    "                            \"type\": \"string\",\n",
    "                            \"description\": \"the name of the printer that the forecast should be sent to\",\n",
    "                            \"enum\": [\"home_printer\", \"office_printer\"] # two printers, just want you to pick one of these\n",
    "                        }\n",
    "                    },\n",
    "                    \"required\": [\"printer_name\"],\n",
    "                },\n",
    "            }\n",
    "        ]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d43c0db",
   "metadata": {},
   "source": [
    "One of the cool parts about OpenAI's new function calls is that the LLM will decide if it should return a normal response to a user, or call the function again. Let's test this out with two different requests in the same query from the user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "1d8033c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_request = \"\"\"\n",
    "Please do three things add 40 units to 2023 headcount\n",
    "and subtract 23 units from 2022 opex\n",
    "then print out the forecast at my home\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "255d1c12",
   "metadata": {},
   "source": [
    "We are going to keep track of the message history ourselves. As more support for function conversations comes in we won't need to do this.\n",
    "\n",
    "First we'll send the message from the user to the LLM along with our function calls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "ddc46318",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='', additional_kwargs={'function_call': {'name': 'edit_financial_forecast', 'arguments': '{\\n  \"year\": 2023,\\n  \"category\": \"headcount\",\\n  \"amount\": 40\\n}'}})"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "first_response = llm.predict_messages([HumanMessage(content=user_request)],\n",
    "                                      functions=function_descriptions)\n",
    "\n",
    "# string the messages together; passing the Human Message, and the functions.\n",
    "\n",
    "first_response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e81d80b",
   "metadata": {},
   "source": [
    "As you can see we get an AIMessage back with no content. However there are `additoinal_kwargs` with the information that we need. Let's pull these out to have a better look at them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "7722f24d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'function_call': {'name': 'edit_financial_forecast',\n",
       "  'arguments': '{\\n  \"year\": 2023,\\n  \"category\": \"headcount\",\\n  \"amount\": 40\\n}'}}"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "first_response.additional_kwargs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "bb754ed6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'edit_financial_forecast'"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function_name = first_response.additional_kwargs[\"function_call\"][\"name\"]\n",
    "function_name"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b25db0b",
   "metadata": {},
   "source": [
    "Then print the arguments it gives back to us"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "172de619",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Year: 2023\n",
      "Category: headcount\n",
      "Amount: 40\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print (f\"\"\"\n",
    "Year: {eval(first_response.additional_kwargs['function_call']['arguments']).get('year')}\n",
    "Category: {eval(first_response.additional_kwargs['function_call']['arguments']).get('category')}\n",
    "Amount: {eval(first_response.additional_kwargs['function_call']['arguments']).get('amount')}\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "912a08ca",
   "metadata": {},
   "source": [
    "But we aren't done! There was a second request in the user query so let's pass it back into the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "daa08ebf",
   "metadata": {},
   "outputs": [],
   "source": [
    "second_response = llm.predict_messages([HumanMessage(content=user_request), # the original query / request\n",
    "                                        AIMessage(content=str(first_response.additional_kwargs)), \n",
    "                                        #^^basically we're telling the language model that they've already responded to the first request.\n",
    "\n",
    "\n",
    "                                        # chat message we specify the function // response from the API\n",
    "                                        ChatMessage(role='function',\n",
    "                                                    additional_kwargs = {'name': function_name},\n",
    "                                                    content = \"Just updated the financial forecast for year 2023, category headcount amd amount 40\"\n",
    "                                                   # ^ content is a phony response; the API said xyz\n",
    "                                                   # \n",
    "                                                   )\n",
    "                                        \n",
    "                                       ],\n",
    "                                       functions=function_descriptions)\n",
    "\n",
    "# after all this is passed to the second response, the AI thinks what it should do next."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a748f6b",
   "metadata": {},
   "source": [
    "Let's see the response from this one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "a015c49e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'function_call': {'name': 'edit_financial_forecast',\n",
       "  'arguments': '{\\n  \"year\": 2022,\\n  \"category\": \"opex\",\\n  \"amount\": -23\\n}'}}"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "second_response.additional_kwargs\n",
    "# you can see that it moved onto the next request"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "41c8bab8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'edit_financial_forecast'"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function_name = second_response.additional_kwargs['function_call']['name']\n",
    "function_name"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74df9449",
   "metadata": {},
   "source": [
    "Cool! It saw that the first response was done and then it went back to our function for us. Let's see what it says if we do it a third time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "ff0cabd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "third_response = llm.predict_messages([HumanMessage(content=user_request),\n",
    "                                       AIMessage(content=str(first_response.additional_kwargs)),\n",
    "                                       AIMessage(content=str(second_response.additional_kwargs)),\n",
    "                                       ChatMessage(role='function',\n",
    "                                                    additional_kwargs = {'name': function_name},\n",
    "                                                    content = \"\"\"\n",
    "                                                        Just made the following updates: 2022, opex -23 and\n",
    "                                                        Year: 2023\n",
    "                                                        Category: headcount\n",
    "                                                        Amount: 40\n",
    "                                                    \"\"\"\n",
    "                                                   )\n",
    "                                       # purposely the content is made sloppy, just to show how much messy/dirty data openAI is able to handle.\n",
    "                                       # but basically, here we are saying, we got these two responses, and the function call response\n",
    "                                       # as above, with the function name, and the content. saying we called function_name, and we got the response\n",
    "                                       # This could be an APi or otherwise.\n",
    "                                       ],\n",
    "                                       functions=function_descriptions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "4ee505df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'function_call': {'name': 'print_financial_forecast',\n",
       "  'arguments': '{\\n  \"printer_name\": \"home_printer\"\\n}'}}"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "third_response.additional_kwargs\n",
    "# function_name = print_financial_forecast\n",
    "# argument = home_printer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "aa33a75c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'print_financial_forecast'"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function_name = third_response.additional_kwargs['function_call']['name']\n",
    "function_name"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea1875f2",
   "metadata": {},
   "source": [
    "Nice! So it knew it was done with the financial forecasts (because we told it so) and then it sent our forecast to our home printer. Let's close it out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "b46f67ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "forth_response = llm.predict_messages([HumanMessage(content=user_request),\n",
    "                                       AIMessage(content=str(first_response.additional_kwargs)),\n",
    "                                       AIMessage(content=str(second_response.additional_kwargs)),\n",
    "                                       AIMessage(content=str(third_response.additional_kwargs)),\n",
    "                                       ChatMessage(role='function',\n",
    "                                                    additional_kwargs = {'name': function_name},\n",
    "                                                    content = \"\"\"\n",
    "                                                        just printed the document at home\n",
    "                                                    \"\"\"\n",
    "                                                   )\n",
    "                                       ],\n",
    "                                       functions=function_descriptions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "0bfd8982",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Great, I've made the changes to your financial forecast and printed it at home for you.\""
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "forth_response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11a9d1e0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
